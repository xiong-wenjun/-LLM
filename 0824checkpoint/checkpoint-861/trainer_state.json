{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 6.947368421052632,
  "eval_steps": 500,
  "global_step": 861,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.04048582995951417,
      "grad_norm": 0.10671006143093109,
      "learning_rate": 4.999869528474403e-05,
      "loss": 0.9034,
      "num_input_tokens_seen": 1096376,
      "step": 5
    },
    {
      "epoch": 0.08097165991902834,
      "grad_norm": 0.11927098780870438,
      "learning_rate": 4.9993395112414575e-05,
      "loss": 0.8892,
      "num_input_tokens_seen": 2151672,
      "step": 10
    },
    {
      "epoch": 0.1214574898785425,
      "grad_norm": 0.07647273689508438,
      "learning_rate": 4.998401880204092e-05,
      "loss": 0.8424,
      "num_input_tokens_seen": 3236048,
      "step": 15
    },
    {
      "epoch": 0.16194331983805668,
      "grad_norm": 0.0628746747970581,
      "learning_rate": 4.997056788279247e-05,
      "loss": 0.8298,
      "num_input_tokens_seen": 4322920,
      "step": 20
    },
    {
      "epoch": 0.20242914979757085,
      "grad_norm": 0.05471624433994293,
      "learning_rate": 4.995304454836095e-05,
      "loss": 0.8444,
      "num_input_tokens_seen": 5365064,
      "step": 25
    },
    {
      "epoch": 0.242914979757085,
      "grad_norm": 0.0452922023832798,
      "learning_rate": 4.993145165660259e-05,
      "loss": 0.8165,
      "num_input_tokens_seen": 6433456,
      "step": 30
    },
    {
      "epoch": 0.2834008097165992,
      "grad_norm": 0.03940761089324951,
      "learning_rate": 4.9905792729072067e-05,
      "loss": 0.7913,
      "num_input_tokens_seen": 7547336,
      "step": 35
    },
    {
      "epoch": 0.32388663967611336,
      "grad_norm": 0.03779703751206398,
      "learning_rate": 4.9876071950448185e-05,
      "loss": 0.7975,
      "num_input_tokens_seen": 8590184,
      "step": 40
    },
    {
      "epoch": 0.3643724696356275,
      "grad_norm": 0.036891791969537735,
      "learning_rate": 4.984229416785139e-05,
      "loss": 0.7858,
      "num_input_tokens_seen": 9671856,
      "step": 45
    },
    {
      "epoch": 0.4048582995951417,
      "grad_norm": 0.03331713750958443,
      "learning_rate": 4.980446489005327e-05,
      "loss": 0.7836,
      "num_input_tokens_seen": 10768280,
      "step": 50
    },
    {
      "epoch": 0.44534412955465585,
      "grad_norm": 0.03105052560567856,
      "learning_rate": 4.976259028657812e-05,
      "loss": 0.7735,
      "num_input_tokens_seen": 11853960,
      "step": 55
    },
    {
      "epoch": 0.48582995951417,
      "grad_norm": 0.035368092358112335,
      "learning_rate": 4.9716677186696756e-05,
      "loss": 0.7772,
      "num_input_tokens_seen": 12923688,
      "step": 60
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 0.03269224241375923,
      "learning_rate": 4.96667330783128e-05,
      "loss": 0.7855,
      "num_input_tokens_seen": 13989888,
      "step": 65
    },
    {
      "epoch": 0.5668016194331984,
      "grad_norm": 0.033456120640039444,
      "learning_rate": 4.9612766106741405e-05,
      "loss": 0.7653,
      "num_input_tokens_seen": 15058520,
      "step": 70
    },
    {
      "epoch": 0.6072874493927125,
      "grad_norm": 0.03490574657917023,
      "learning_rate": 4.9554785073380905e-05,
      "loss": 0.7614,
      "num_input_tokens_seen": 16112000,
      "step": 75
    },
    {
      "epoch": 0.6477732793522267,
      "grad_norm": 0.03678217902779579,
      "learning_rate": 4.9492799434277375e-05,
      "loss": 0.7723,
      "num_input_tokens_seen": 17189048,
      "step": 80
    },
    {
      "epoch": 0.6882591093117408,
      "grad_norm": 0.03654978796839714,
      "learning_rate": 4.9426819298582486e-05,
      "loss": 0.7734,
      "num_input_tokens_seen": 18246992,
      "step": 85
    },
    {
      "epoch": 0.728744939271255,
      "grad_norm": 0.03823176398873329,
      "learning_rate": 4.935685542690478e-05,
      "loss": 0.7459,
      "num_input_tokens_seen": 19313568,
      "step": 90
    },
    {
      "epoch": 0.7692307692307693,
      "grad_norm": 0.037594668567180634,
      "learning_rate": 4.9282919229554786e-05,
      "loss": 0.746,
      "num_input_tokens_seen": 20366624,
      "step": 95
    },
    {
      "epoch": 0.8097165991902834,
      "grad_norm": 0.03907329589128494,
      "learning_rate": 4.9205022764684087e-05,
      "loss": 0.7532,
      "num_input_tokens_seen": 21416720,
      "step": 100
    },
    {
      "epoch": 0.8502024291497976,
      "grad_norm": 0.04053748771548271,
      "learning_rate": 4.912317873631877e-05,
      "loss": 0.7601,
      "num_input_tokens_seen": 22471032,
      "step": 105
    },
    {
      "epoch": 0.8906882591093117,
      "grad_norm": 0.04244143143296242,
      "learning_rate": 4.90374004922876e-05,
      "loss": 0.7417,
      "num_input_tokens_seen": 23546592,
      "step": 110
    },
    {
      "epoch": 0.9311740890688259,
      "grad_norm": 0.039729729294776917,
      "learning_rate": 4.894770202204508e-05,
      "loss": 0.7346,
      "num_input_tokens_seen": 24654256,
      "step": 115
    },
    {
      "epoch": 0.97165991902834,
      "grad_norm": 0.04494675248861313,
      "learning_rate": 4.885409795438995e-05,
      "loss": 0.751,
      "num_input_tokens_seen": 25717816,
      "step": 120
    },
    {
      "epoch": 1.008097165991903,
      "grad_norm": 0.06784409284591675,
      "learning_rate": 4.875660355507941e-05,
      "loss": 0.7372,
      "num_input_tokens_seen": 26709088,
      "step": 125
    },
    {
      "epoch": 1.048582995951417,
      "grad_norm": 0.04887770861387253,
      "learning_rate": 4.865523472433941e-05,
      "loss": 0.7469,
      "num_input_tokens_seen": 27786624,
      "step": 130
    },
    {
      "epoch": 1.0890688259109311,
      "grad_norm": 0.04687180370092392,
      "learning_rate": 4.8550007994271594e-05,
      "loss": 0.719,
      "num_input_tokens_seen": 28857728,
      "step": 135
    },
    {
      "epoch": 1.1295546558704452,
      "grad_norm": 0.04837200418114662,
      "learning_rate": 4.8440940526156973e-05,
      "loss": 0.7306,
      "num_input_tokens_seen": 29934064,
      "step": 140
    },
    {
      "epoch": 1.1700404858299596,
      "grad_norm": 0.052863262593746185,
      "learning_rate": 4.8328050107657236e-05,
      "loss": 0.7291,
      "num_input_tokens_seen": 30982480,
      "step": 145
    },
    {
      "epoch": 1.2105263157894737,
      "grad_norm": 0.05117516964673996,
      "learning_rate": 4.821135514991369e-05,
      "loss": 0.7377,
      "num_input_tokens_seen": 32024384,
      "step": 150
    },
    {
      "epoch": 1.2510121457489878,
      "grad_norm": 0.05257982760667801,
      "learning_rate": 4.80908746845447e-05,
      "loss": 0.7205,
      "num_input_tokens_seen": 33083088,
      "step": 155
    },
    {
      "epoch": 1.291497975708502,
      "grad_norm": 0.05194094777107239,
      "learning_rate": 4.7966628360541755e-05,
      "loss": 0.7292,
      "num_input_tokens_seen": 34160528,
      "step": 160
    },
    {
      "epoch": 1.3319838056680162,
      "grad_norm": 0.05315370112657547,
      "learning_rate": 4.783863644106502e-05,
      "loss": 0.7319,
      "num_input_tokens_seen": 35205816,
      "step": 165
    },
    {
      "epoch": 1.3724696356275303,
      "grad_norm": 0.054596662521362305,
      "learning_rate": 4.7706919800138636e-05,
      "loss": 0.7211,
      "num_input_tokens_seen": 36301824,
      "step": 170
    },
    {
      "epoch": 1.4129554655870447,
      "grad_norm": 0.06218675896525383,
      "learning_rate": 4.757149991924633e-05,
      "loss": 0.7359,
      "num_input_tokens_seen": 37375600,
      "step": 175
    },
    {
      "epoch": 1.4534412955465588,
      "grad_norm": 0.05349303036928177,
      "learning_rate": 4.7432398883828124e-05,
      "loss": 0.7175,
      "num_input_tokens_seen": 38468864,
      "step": 180
    },
    {
      "epoch": 1.4939271255060729,
      "grad_norm": 0.0583990216255188,
      "learning_rate": 4.728963937967841e-05,
      "loss": 0.7238,
      "num_input_tokens_seen": 39535896,
      "step": 185
    },
    {
      "epoch": 1.5344129554655872,
      "grad_norm": 0.06224983185529709,
      "learning_rate": 4.714324468924614e-05,
      "loss": 0.7177,
      "num_input_tokens_seen": 40581280,
      "step": 190
    },
    {
      "epoch": 1.574898785425101,
      "grad_norm": 0.05747327581048012,
      "learning_rate": 4.6993238687837746e-05,
      "loss": 0.7249,
      "num_input_tokens_seen": 41633064,
      "step": 195
    },
    {
      "epoch": 1.6153846153846154,
      "grad_norm": 0.05855716019868851,
      "learning_rate": 4.683964583972336e-05,
      "loss": 0.7103,
      "num_input_tokens_seen": 42722384,
      "step": 200
    },
    {
      "epoch": 1.6558704453441295,
      "grad_norm": 0.05747295916080475,
      "learning_rate": 4.668249119414692e-05,
      "loss": 0.72,
      "num_input_tokens_seen": 43815712,
      "step": 205
    },
    {
      "epoch": 1.6963562753036436,
      "grad_norm": 0.06081166863441467,
      "learning_rate": 4.652180038124098e-05,
      "loss": 0.7165,
      "num_input_tokens_seen": 44876176,
      "step": 210
    },
    {
      "epoch": 1.736842105263158,
      "grad_norm": 0.057800207287073135,
      "learning_rate": 4.635759960784672e-05,
      "loss": 0.7135,
      "num_input_tokens_seen": 45965632,
      "step": 215
    },
    {
      "epoch": 1.777327935222672,
      "grad_norm": 0.05726437643170357,
      "learning_rate": 4.618991565323987e-05,
      "loss": 0.7024,
      "num_input_tokens_seen": 47049864,
      "step": 220
    },
    {
      "epoch": 1.8178137651821862,
      "grad_norm": 0.06477554887533188,
      "learning_rate": 4.601877586476333e-05,
      "loss": 0.709,
      "num_input_tokens_seen": 48140256,
      "step": 225
    },
    {
      "epoch": 1.8582995951417005,
      "grad_norm": 0.06099247932434082,
      "learning_rate": 4.5844208153367186e-05,
      "loss": 0.7178,
      "num_input_tokens_seen": 49219632,
      "step": 230
    },
    {
      "epoch": 1.8987854251012146,
      "grad_norm": 0.06155874952673912,
      "learning_rate": 4.566624098905665e-05,
      "loss": 0.7043,
      "num_input_tokens_seen": 50296776,
      "step": 235
    },
    {
      "epoch": 1.9392712550607287,
      "grad_norm": 0.06411747634410858,
      "learning_rate": 4.548490339624901e-05,
      "loss": 0.6996,
      "num_input_tokens_seen": 51434328,
      "step": 240
    },
    {
      "epoch": 1.979757085020243,
      "grad_norm": 0.06212175264954567,
      "learning_rate": 4.530022494904005e-05,
      "loss": 0.6956,
      "num_input_tokens_seen": 52481896,
      "step": 245
    },
    {
      "epoch": 2.016194331983806,
      "grad_norm": 0.07535071671009064,
      "learning_rate": 4.511223576638084e-05,
      "loss": 0.6998,
      "num_input_tokens_seen": 53431360,
      "step": 250
    },
    {
      "epoch": 2.0566801619433197,
      "grad_norm": 0.07738073915243149,
      "learning_rate": 4.492096650716569e-05,
      "loss": 0.7162,
      "num_input_tokens_seen": 54459720,
      "step": 255
    },
    {
      "epoch": 2.097165991902834,
      "grad_norm": 0.07530993223190308,
      "learning_rate": 4.4726448365232066e-05,
      "loss": 0.6866,
      "num_input_tokens_seen": 55561240,
      "step": 260
    },
    {
      "epoch": 2.1376518218623484,
      "grad_norm": 0.08103297650814056,
      "learning_rate": 4.452871306427314e-05,
      "loss": 0.7056,
      "num_input_tokens_seen": 56616440,
      "step": 265
    },
    {
      "epoch": 2.1781376518218623,
      "grad_norm": 0.07233437895774841,
      "learning_rate": 4.432779285266411e-05,
      "loss": 0.7011,
      "num_input_tokens_seen": 57679544,
      "step": 270
    },
    {
      "epoch": 2.2186234817813766,
      "grad_norm": 0.0686030387878418,
      "learning_rate": 4.4123720498202825e-05,
      "loss": 0.7059,
      "num_input_tokens_seen": 58714752,
      "step": 275
    },
    {
      "epoch": 2.2591093117408905,
      "grad_norm": 0.0747467502951622,
      "learning_rate": 4.391652928276572e-05,
      "loss": 0.6939,
      "num_input_tokens_seen": 59809984,
      "step": 280
    },
    {
      "epoch": 2.299595141700405,
      "grad_norm": 0.07244465500116348,
      "learning_rate": 4.3706252996879914e-05,
      "loss": 0.6931,
      "num_input_tokens_seen": 60891976,
      "step": 285
    },
    {
      "epoch": 2.340080971659919,
      "grad_norm": 0.06944893300533295,
      "learning_rate": 4.3492925934212404e-05,
      "loss": 0.6923,
      "num_input_tokens_seen": 62024712,
      "step": 290
    },
    {
      "epoch": 2.380566801619433,
      "grad_norm": 0.07264506071805954,
      "learning_rate": 4.32765828859771e-05,
      "loss": 0.6944,
      "num_input_tokens_seen": 63108016,
      "step": 295
    },
    {
      "epoch": 2.4210526315789473,
      "grad_norm": 0.06920690089464188,
      "learning_rate": 4.305725913526082e-05,
      "loss": 0.7129,
      "num_input_tokens_seen": 64202832,
      "step": 300
    },
    {
      "epoch": 2.4615384615384617,
      "grad_norm": 0.0698445737361908,
      "learning_rate": 4.2834990451269e-05,
      "loss": 0.6874,
      "num_input_tokens_seen": 65281968,
      "step": 305
    },
    {
      "epoch": 2.5020242914979756,
      "grad_norm": 0.07270193845033646,
      "learning_rate": 4.2609813083492135e-05,
      "loss": 0.7057,
      "num_input_tokens_seen": 66356216,
      "step": 310
    },
    {
      "epoch": 2.54251012145749,
      "grad_norm": 0.09156140685081482,
      "learning_rate": 4.238176375579394e-05,
      "loss": 0.6868,
      "num_input_tokens_seen": 67438952,
      "step": 315
    },
    {
      "epoch": 2.582995951417004,
      "grad_norm": 0.07627272605895996,
      "learning_rate": 4.215087966042206e-05,
      "loss": 0.7119,
      "num_input_tokens_seen": 68487952,
      "step": 320
    },
    {
      "epoch": 2.623481781376518,
      "grad_norm": 0.07614287734031677,
      "learning_rate": 4.191719845194245e-05,
      "loss": 0.6937,
      "num_input_tokens_seen": 69556520,
      "step": 325
    },
    {
      "epoch": 2.6639676113360324,
      "grad_norm": 0.07495689392089844,
      "learning_rate": 4.1680758241098375e-05,
      "loss": 0.6903,
      "num_input_tokens_seen": 70609216,
      "step": 330
    },
    {
      "epoch": 2.7044534412955468,
      "grad_norm": 0.07454993575811386,
      "learning_rate": 4.144159758859494e-05,
      "loss": 0.699,
      "num_input_tokens_seen": 71677240,
      "step": 335
    },
    {
      "epoch": 2.7449392712550607,
      "grad_norm": 0.07550632208585739,
      "learning_rate": 4.119975549881029e-05,
      "loss": 0.6864,
      "num_input_tokens_seen": 72789152,
      "step": 340
    },
    {
      "epoch": 2.785425101214575,
      "grad_norm": 0.08117334544658661,
      "learning_rate": 4.095527141343446e-05,
      "loss": 0.693,
      "num_input_tokens_seen": 73849160,
      "step": 345
    },
    {
      "epoch": 2.8259109311740893,
      "grad_norm": 0.07738247513771057,
      "learning_rate": 4.070818520503688e-05,
      "loss": 0.6959,
      "num_input_tokens_seen": 74939000,
      "step": 350
    },
    {
      "epoch": 2.866396761133603,
      "grad_norm": 0.0804910734295845,
      "learning_rate": 4.045853717056358e-05,
      "loss": 0.6967,
      "num_input_tokens_seen": 76005816,
      "step": 355
    },
    {
      "epoch": 2.9068825910931175,
      "grad_norm": 0.07703825831413269,
      "learning_rate": 4.020636802476525e-05,
      "loss": 0.696,
      "num_input_tokens_seen": 77097848,
      "step": 360
    },
    {
      "epoch": 2.9473684210526314,
      "grad_norm": 0.07888875156641006,
      "learning_rate": 3.9951718893557135e-05,
      "loss": 0.6957,
      "num_input_tokens_seen": 78113296,
      "step": 365
    },
    {
      "epoch": 2.9878542510121457,
      "grad_norm": 0.08158689737319946,
      "learning_rate": 3.969463130731183e-05,
      "loss": 0.696,
      "num_input_tokens_seen": 79186616,
      "step": 370
    },
    {
      "epoch": 3.0242914979757085,
      "grad_norm": 0.07737667113542557,
      "learning_rate": 3.943514719408618e-05,
      "loss": 0.697,
      "num_input_tokens_seen": 80129384,
      "step": 375
    },
    {
      "epoch": 3.064777327935223,
      "grad_norm": 0.08349259197711945,
      "learning_rate": 3.9173308872783286e-05,
      "loss": 0.6827,
      "num_input_tokens_seen": 81219072,
      "step": 380
    },
    {
      "epoch": 3.1052631578947367,
      "grad_norm": 0.08041991293430328,
      "learning_rate": 3.8909159046250755e-05,
      "loss": 0.6772,
      "num_input_tokens_seen": 82335992,
      "step": 385
    },
    {
      "epoch": 3.145748987854251,
      "grad_norm": 0.08971772342920303,
      "learning_rate": 3.864274079431638e-05,
      "loss": 0.6944,
      "num_input_tokens_seen": 83419112,
      "step": 390
    },
    {
      "epoch": 3.1862348178137654,
      "grad_norm": 0.07567635923624039,
      "learning_rate": 3.837409756676229e-05,
      "loss": 0.6868,
      "num_input_tokens_seen": 84486768,
      "step": 395
    },
    {
      "epoch": 3.2267206477732793,
      "grad_norm": 0.08132917433977127,
      "learning_rate": 3.810327317623881e-05,
      "loss": 0.6803,
      "num_input_tokens_seen": 85552640,
      "step": 400
    },
    {
      "epoch": 3.2672064777327936,
      "grad_norm": 0.08678089827299118,
      "learning_rate": 3.783031179111907e-05,
      "loss": 0.6893,
      "num_input_tokens_seen": 86603512,
      "step": 405
    },
    {
      "epoch": 3.3076923076923075,
      "grad_norm": 0.08810210227966309,
      "learning_rate": 3.755525792829569e-05,
      "loss": 0.6877,
      "num_input_tokens_seen": 87675328,
      "step": 410
    },
    {
      "epoch": 3.348178137651822,
      "grad_norm": 0.0823664665222168,
      "learning_rate": 3.727815644592058e-05,
      "loss": 0.7003,
      "num_input_tokens_seen": 88691120,
      "step": 415
    },
    {
      "epoch": 3.388663967611336,
      "grad_norm": 0.08383727073669434,
      "learning_rate": 3.699905253608907e-05,
      "loss": 0.6843,
      "num_input_tokens_seen": 89785480,
      "step": 420
    },
    {
      "epoch": 3.42914979757085,
      "grad_norm": 0.08696340769529343,
      "learning_rate": 3.671799171746958e-05,
      "loss": 0.6953,
      "num_input_tokens_seen": 90851744,
      "step": 425
    },
    {
      "epoch": 3.4696356275303644,
      "grad_norm": 0.07914034277200699,
      "learning_rate": 3.6435019827880094e-05,
      "loss": 0.6833,
      "num_input_tokens_seen": 91928488,
      "step": 430
    },
    {
      "epoch": 3.5101214574898787,
      "grad_norm": 0.09376419335603714,
      "learning_rate": 3.6150183016812464e-05,
      "loss": 0.689,
      "num_input_tokens_seen": 92989456,
      "step": 435
    },
    {
      "epoch": 3.5506072874493926,
      "grad_norm": 0.08967117220163345,
      "learning_rate": 3.5863527737906013e-05,
      "loss": 0.6861,
      "num_input_tokens_seen": 94048352,
      "step": 440
    },
    {
      "epoch": 3.591093117408907,
      "grad_norm": 0.08396787196397781,
      "learning_rate": 3.557510074137147e-05,
      "loss": 0.6801,
      "num_input_tokens_seen": 95156560,
      "step": 445
    },
    {
      "epoch": 3.6315789473684212,
      "grad_norm": 0.09065305441617966,
      "learning_rate": 3.52849490663665e-05,
      "loss": 0.6848,
      "num_input_tokens_seen": 96198864,
      "step": 450
    },
    {
      "epoch": 3.672064777327935,
      "grad_norm": 0.0832642987370491,
      "learning_rate": 3.49931200333242e-05,
      "loss": 0.6945,
      "num_input_tokens_seen": 97278888,
      "step": 455
    },
    {
      "epoch": 3.7125506072874495,
      "grad_norm": 0.09451127797365189,
      "learning_rate": 3.469966123623563e-05,
      "loss": 0.6914,
      "num_input_tokens_seen": 98344472,
      "step": 460
    },
    {
      "epoch": 3.753036437246964,
      "grad_norm": 0.0942223072052002,
      "learning_rate": 3.4404620534887836e-05,
      "loss": 0.6837,
      "num_input_tokens_seen": 99413416,
      "step": 465
    },
    {
      "epoch": 3.7935222672064777,
      "grad_norm": 0.0849640890955925,
      "learning_rate": 3.410804604705842e-05,
      "loss": 0.6664,
      "num_input_tokens_seen": 100499912,
      "step": 470
    },
    {
      "epoch": 3.834008097165992,
      "grad_norm": 0.0929107740521431,
      "learning_rate": 3.380998614066805e-05,
      "loss": 0.6784,
      "num_input_tokens_seen": 101597688,
      "step": 475
    },
    {
      "epoch": 3.8744939271255063,
      "grad_norm": 0.0869821161031723,
      "learning_rate": 3.351048942589229e-05,
      "loss": 0.6673,
      "num_input_tokens_seen": 102731248,
      "step": 480
    },
    {
      "epoch": 3.91497975708502,
      "grad_norm": 0.08458308130502701,
      "learning_rate": 3.3209604747233776e-05,
      "loss": 0.6707,
      "num_input_tokens_seen": 103804296,
      "step": 485
    },
    {
      "epoch": 3.9554655870445345,
      "grad_norm": 0.09130382537841797,
      "learning_rate": 3.2907381175556215e-05,
      "loss": 0.6784,
      "num_input_tokens_seen": 104874312,
      "step": 490
    },
    {
      "epoch": 3.9959514170040484,
      "grad_norm": 0.08880870789289474,
      "learning_rate": 3.2603868000081546e-05,
      "loss": 0.6911,
      "num_input_tokens_seen": 105904776,
      "step": 495
    },
    {
      "epoch": 4.032388663967612,
      "grad_norm": 0.09884089231491089,
      "learning_rate": 3.229911472035137e-05,
      "loss": 0.6786,
      "num_input_tokens_seen": 106856520,
      "step": 500
    },
    {
      "epoch": 4.0728744939271255,
      "grad_norm": 0.09142468124628067,
      "learning_rate": 3.1993171038154204e-05,
      "loss": 0.6913,
      "num_input_tokens_seen": 107890088,
      "step": 505
    },
    {
      "epoch": 4.113360323886639,
      "grad_norm": 0.08758687973022461,
      "learning_rate": 3.16860868494196e-05,
      "loss": 0.6706,
      "num_input_tokens_seen": 108960944,
      "step": 510
    },
    {
      "epoch": 4.153846153846154,
      "grad_norm": 0.09157994389533997,
      "learning_rate": 3.1377912236080764e-05,
      "loss": 0.6776,
      "num_input_tokens_seen": 110029280,
      "step": 515
    },
    {
      "epoch": 4.194331983805668,
      "grad_norm": 0.10011561959981918,
      "learning_rate": 3.106869745790674e-05,
      "loss": 0.6729,
      "num_input_tokens_seen": 111092696,
      "step": 520
    },
    {
      "epoch": 4.234817813765182,
      "grad_norm": 0.09581475704908371,
      "learning_rate": 3.0758492944305586e-05,
      "loss": 0.6843,
      "num_input_tokens_seen": 112148120,
      "step": 525
    },
    {
      "epoch": 4.275303643724697,
      "grad_norm": 0.09694772213697433,
      "learning_rate": 3.0447349286099947e-05,
      "loss": 0.6731,
      "num_input_tokens_seen": 113210816,
      "step": 530
    },
    {
      "epoch": 4.315789473684211,
      "grad_norm": 0.09578830748796463,
      "learning_rate": 3.0135317227276245e-05,
      "loss": 0.6877,
      "num_input_tokens_seen": 114257696,
      "step": 535
    },
    {
      "epoch": 4.3562753036437245,
      "grad_norm": 0.09495487809181213,
      "learning_rate": 2.9822447656708946e-05,
      "loss": 0.6796,
      "num_input_tokens_seen": 115353008,
      "step": 540
    },
    {
      "epoch": 4.396761133603239,
      "grad_norm": 0.09112279862165451,
      "learning_rate": 2.950879159986113e-05,
      "loss": 0.6742,
      "num_input_tokens_seen": 116388584,
      "step": 545
    },
    {
      "epoch": 4.437246963562753,
      "grad_norm": 0.09295309334993362,
      "learning_rate": 2.9194400210462808e-05,
      "loss": 0.6778,
      "num_input_tokens_seen": 117470328,
      "step": 550
    },
    {
      "epoch": 4.477732793522267,
      "grad_norm": 0.09773346036672592,
      "learning_rate": 2.887932476216838e-05,
      "loss": 0.6845,
      "num_input_tokens_seen": 118568512,
      "step": 555
    },
    {
      "epoch": 4.518218623481781,
      "grad_norm": 0.09844498336315155,
      "learning_rate": 2.8563616640194457e-05,
      "loss": 0.6616,
      "num_input_tokens_seen": 119637488,
      "step": 560
    },
    {
      "epoch": 4.558704453441296,
      "grad_norm": 0.139826238155365,
      "learning_rate": 2.824732733293951e-05,
      "loss": 0.679,
      "num_input_tokens_seen": 120705712,
      "step": 565
    },
    {
      "epoch": 4.59919028340081,
      "grad_norm": 0.10038987547159195,
      "learning_rate": 2.7930508423586728e-05,
      "loss": 0.6712,
      "num_input_tokens_seen": 121790872,
      "step": 570
    },
    {
      "epoch": 4.6396761133603235,
      "grad_norm": 0.09864530712366104,
      "learning_rate": 2.761321158169134e-05,
      "loss": 0.6643,
      "num_input_tokens_seen": 122883128,
      "step": 575
    },
    {
      "epoch": 4.680161943319838,
      "grad_norm": 0.11165226250886917,
      "learning_rate": 2.7295488554753956e-05,
      "loss": 0.6698,
      "num_input_tokens_seen": 123966968,
      "step": 580
    },
    {
      "epoch": 4.720647773279352,
      "grad_norm": 0.09801849722862244,
      "learning_rate": 2.6977391159781096e-05,
      "loss": 0.6744,
      "num_input_tokens_seen": 125060928,
      "step": 585
    },
    {
      "epoch": 4.761133603238866,
      "grad_norm": 0.09559643268585205,
      "learning_rate": 2.6658971274834443e-05,
      "loss": 0.6742,
      "num_input_tokens_seen": 126153016,
      "step": 590
    },
    {
      "epoch": 4.801619433198381,
      "grad_norm": 0.09712309390306473,
      "learning_rate": 2.634028083057014e-05,
      "loss": 0.6773,
      "num_input_tokens_seen": 127229640,
      "step": 595
    },
    {
      "epoch": 4.842105263157895,
      "grad_norm": 0.09394094347953796,
      "learning_rate": 2.6021371801769447e-05,
      "loss": 0.6805,
      "num_input_tokens_seen": 128310136,
      "step": 600
    },
    {
      "epoch": 4.882591093117409,
      "grad_norm": 0.09639731049537659,
      "learning_rate": 2.5702296198862284e-05,
      "loss": 0.6821,
      "num_input_tokens_seen": 129393800,
      "step": 605
    },
    {
      "epoch": 4.923076923076923,
      "grad_norm": 0.09915004670619965,
      "learning_rate": 2.5383106059444912e-05,
      "loss": 0.6786,
      "num_input_tokens_seen": 130492880,
      "step": 610
    },
    {
      "epoch": 4.963562753036437,
      "grad_norm": 0.09776534885168076,
      "learning_rate": 2.5063853439793185e-05,
      "loss": 0.6716,
      "num_input_tokens_seen": 131557168,
      "step": 615
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.10442228615283966,
      "learning_rate": 2.474459040637278e-05,
      "loss": 0.6836,
      "num_input_tokens_seen": 132525800,
      "step": 620
    },
    {
      "epoch": 5.040485829959514,
      "grad_norm": 0.10558006912469864,
      "learning_rate": 2.4425369027347743e-05,
      "loss": 0.6783,
      "num_input_tokens_seen": 133605544,
      "step": 625
    },
    {
      "epoch": 5.080971659919029,
      "grad_norm": 0.09800350666046143,
      "learning_rate": 2.4106241364088746e-05,
      "loss": 0.671,
      "num_input_tokens_seen": 134669112,
      "step": 630
    },
    {
      "epoch": 5.1214574898785425,
      "grad_norm": 0.0992128998041153,
      "learning_rate": 2.3787259462682494e-05,
      "loss": 0.6694,
      "num_input_tokens_seen": 135703544,
      "step": 635
    },
    {
      "epoch": 5.161943319838056,
      "grad_norm": 0.0981452539563179,
      "learning_rate": 2.346847534544362e-05,
      "loss": 0.6701,
      "num_input_tokens_seen": 136794064,
      "step": 640
    },
    {
      "epoch": 5.202429149797571,
      "grad_norm": 0.09935453534126282,
      "learning_rate": 2.3149941002430367e-05,
      "loss": 0.6749,
      "num_input_tokens_seen": 137831136,
      "step": 645
    },
    {
      "epoch": 5.242914979757085,
      "grad_norm": 0.09427853673696518,
      "learning_rate": 2.2831708382965688e-05,
      "loss": 0.6723,
      "num_input_tokens_seen": 138883984,
      "step": 650
    },
    {
      "epoch": 5.283400809716599,
      "grad_norm": 0.10736090689897537,
      "learning_rate": 2.2513829387164822e-05,
      "loss": 0.665,
      "num_input_tokens_seen": 139954360,
      "step": 655
    },
    {
      "epoch": 5.323886639676114,
      "grad_norm": 0.10160353034734726,
      "learning_rate": 2.219635585747107e-05,
      "loss": 0.67,
      "num_input_tokens_seen": 141020616,
      "step": 660
    },
    {
      "epoch": 5.364372469635628,
      "grad_norm": 0.10267932713031769,
      "learning_rate": 2.187933957020077e-05,
      "loss": 0.6678,
      "num_input_tokens_seen": 142097624,
      "step": 665
    },
    {
      "epoch": 5.4048582995951415,
      "grad_norm": 0.10223177075386047,
      "learning_rate": 2.1562832227099264e-05,
      "loss": 0.6641,
      "num_input_tokens_seen": 143216848,
      "step": 670
    },
    {
      "epoch": 5.445344129554655,
      "grad_norm": 0.09316312521696091,
      "learning_rate": 2.124688544690891e-05,
      "loss": 0.6669,
      "num_input_tokens_seen": 144321368,
      "step": 675
    },
    {
      "epoch": 5.48582995951417,
      "grad_norm": 0.09852235019207001,
      "learning_rate": 2.0931550756950598e-05,
      "loss": 0.663,
      "num_input_tokens_seen": 145437936,
      "step": 680
    },
    {
      "epoch": 5.526315789473684,
      "grad_norm": 0.10493888705968857,
      "learning_rate": 2.0616879584720306e-05,
      "loss": 0.6929,
      "num_input_tokens_seen": 146487520,
      "step": 685
    },
    {
      "epoch": 5.566801619433198,
      "grad_norm": 0.10092651844024658,
      "learning_rate": 2.0302923249501826e-05,
      "loss": 0.6715,
      "num_input_tokens_seen": 147579080,
      "step": 690
    },
    {
      "epoch": 5.607287449392713,
      "grad_norm": 0.09793562442064285,
      "learning_rate": 1.9989732953997166e-05,
      "loss": 0.6747,
      "num_input_tokens_seen": 148635872,
      "step": 695
    },
    {
      "epoch": 5.647773279352227,
      "grad_norm": 0.10664958506822586,
      "learning_rate": 1.967735977597598e-05,
      "loss": 0.6724,
      "num_input_tokens_seen": 149698136,
      "step": 700
    },
    {
      "epoch": 5.6882591093117405,
      "grad_norm": 0.10632201284170151,
      "learning_rate": 1.9365854659945376e-05,
      "loss": 0.6753,
      "num_input_tokens_seen": 150758632,
      "step": 705
    },
    {
      "epoch": 5.728744939271255,
      "grad_norm": 0.09911195188760757,
      "learning_rate": 1.905526840884145e-05,
      "loss": 0.6728,
      "num_input_tokens_seen": 151846584,
      "step": 710
    },
    {
      "epoch": 5.769230769230769,
      "grad_norm": 0.09907060116529465,
      "learning_rate": 1.8745651675743876e-05,
      "loss": 0.6553,
      "num_input_tokens_seen": 152908488,
      "step": 715
    },
    {
      "epoch": 5.809716599190283,
      "grad_norm": 0.10095091164112091,
      "learning_rate": 1.8437054955614976e-05,
      "loss": 0.6702,
      "num_input_tokens_seen": 153955384,
      "step": 720
    },
    {
      "epoch": 5.850202429149798,
      "grad_norm": 0.10004233568906784,
      "learning_rate": 1.8129528577064614e-05,
      "loss": 0.6675,
      "num_input_tokens_seen": 155031768,
      "step": 725
    },
    {
      "epoch": 5.890688259109312,
      "grad_norm": 0.1053246259689331,
      "learning_rate": 1.7823122694142107e-05,
      "loss": 0.6533,
      "num_input_tokens_seen": 156135312,
      "step": 730
    },
    {
      "epoch": 5.931174089068826,
      "grad_norm": 0.10858117043972015,
      "learning_rate": 1.7517887278156694e-05,
      "loss": 0.6926,
      "num_input_tokens_seen": 157151088,
      "step": 735
    },
    {
      "epoch": 5.97165991902834,
      "grad_norm": 0.10051840543746948,
      "learning_rate": 1.7213872109527812e-05,
      "loss": 0.6657,
      "num_input_tokens_seen": 158250040,
      "step": 740
    },
    {
      "epoch": 6.008097165991903,
      "grad_norm": 0.09820017218589783,
      "learning_rate": 1.691112676966644e-05,
      "loss": 0.6599,
      "num_input_tokens_seen": 159249064,
      "step": 745
    },
    {
      "epoch": 6.048582995951417,
      "grad_norm": 0.10443828999996185,
      "learning_rate": 1.6609700632888952e-05,
      "loss": 0.6695,
      "num_input_tokens_seen": 160346568,
      "step": 750
    },
    {
      "epoch": 6.089068825910931,
      "grad_norm": 0.09883162379264832,
      "learning_rate": 1.630964285836471e-05,
      "loss": 0.6556,
      "num_input_tokens_seen": 161429272,
      "step": 755
    },
    {
      "epoch": 6.129554655870446,
      "grad_norm": 0.09973645210266113,
      "learning_rate": 1.6011002382098804e-05,
      "loss": 0.6752,
      "num_input_tokens_seen": 162480544,
      "step": 760
    },
    {
      "epoch": 6.17004048582996,
      "grad_norm": 0.10203347355127335,
      "learning_rate": 1.5713827908951123e-05,
      "loss": 0.6792,
      "num_input_tokens_seen": 163522288,
      "step": 765
    },
    {
      "epoch": 6.2105263157894735,
      "grad_norm": 0.10686712712049484,
      "learning_rate": 1.541816790469312e-05,
      "loss": 0.6578,
      "num_input_tokens_seen": 164629136,
      "step": 770
    },
    {
      "epoch": 6.251012145748988,
      "grad_norm": 0.10569338500499725,
      "learning_rate": 1.5124070588103648e-05,
      "loss": 0.6765,
      "num_input_tokens_seen": 165657160,
      "step": 775
    },
    {
      "epoch": 6.291497975708502,
      "grad_norm": 0.11870934069156647,
      "learning_rate": 1.4831583923104999e-05,
      "loss": 0.675,
      "num_input_tokens_seen": 166716600,
      "step": 780
    },
    {
      "epoch": 6.331983805668016,
      "grad_norm": 0.10125405341386795,
      "learning_rate": 1.4540755610940514e-05,
      "loss": 0.6672,
      "num_input_tokens_seen": 167813840,
      "step": 785
    },
    {
      "epoch": 6.372469635627531,
      "grad_norm": 0.1106375977396965,
      "learning_rate": 1.4251633082395116e-05,
      "loss": 0.663,
      "num_input_tokens_seen": 168881672,
      "step": 790
    },
    {
      "epoch": 6.412955465587045,
      "grad_norm": 0.10465242713689804,
      "learning_rate": 1.3964263490059826e-05,
      "loss": 0.6646,
      "num_input_tokens_seen": 169950952,
      "step": 795
    },
    {
      "epoch": 6.4534412955465585,
      "grad_norm": 0.10822275280952454,
      "learning_rate": 1.3678693700641723e-05,
      "loss": 0.6654,
      "num_input_tokens_seen": 171001048,
      "step": 800
    },
    {
      "epoch": 6.493927125506072,
      "grad_norm": 0.1080651506781578,
      "learning_rate": 1.3394970287320551e-05,
      "loss": 0.6643,
      "num_input_tokens_seen": 172110376,
      "step": 805
    },
    {
      "epoch": 6.534412955465587,
      "grad_norm": 0.10612118989229202,
      "learning_rate": 1.311313952215317e-05,
      "loss": 0.6783,
      "num_input_tokens_seen": 173161312,
      "step": 810
    },
    {
      "epoch": 6.574898785425101,
      "grad_norm": 0.1068553552031517,
      "learning_rate": 1.283324736852703e-05,
      "loss": 0.6659,
      "num_input_tokens_seen": 174250152,
      "step": 815
    },
    {
      "epoch": 6.615384615384615,
      "grad_norm": 0.10770104080438614,
      "learning_rate": 1.2555339473664151e-05,
      "loss": 0.6754,
      "num_input_tokens_seen": 175275176,
      "step": 820
    },
    {
      "epoch": 6.65587044534413,
      "grad_norm": 0.10256513953208923,
      "learning_rate": 1.2279461161176558e-05,
      "loss": 0.6593,
      "num_input_tokens_seen": 176362080,
      "step": 825
    },
    {
      "epoch": 6.696356275303644,
      "grad_norm": 0.09931151568889618,
      "learning_rate": 1.2005657423674496e-05,
      "loss": 0.6509,
      "num_input_tokens_seen": 177443048,
      "step": 830
    },
    {
      "epoch": 6.7368421052631575,
      "grad_norm": 0.09949983656406403,
      "learning_rate": 1.1733972915428665e-05,
      "loss": 0.6639,
      "num_input_tokens_seen": 178500944,
      "step": 835
    },
    {
      "epoch": 6.777327935222672,
      "grad_norm": 0.10378285497426987,
      "learning_rate": 1.1464451945087617e-05,
      "loss": 0.6677,
      "num_input_tokens_seen": 179565264,
      "step": 840
    },
    {
      "epoch": 6.817813765182186,
      "grad_norm": 0.10360514372587204,
      "learning_rate": 1.1197138468451513e-05,
      "loss": 0.6747,
      "num_input_tokens_seen": 180616136,
      "step": 845
    },
    {
      "epoch": 6.8582995951417,
      "grad_norm": 0.10315947234630585,
      "learning_rate": 1.0932076081303441e-05,
      "loss": 0.6728,
      "num_input_tokens_seen": 181673464,
      "step": 850
    },
    {
      "epoch": 6.898785425101215,
      "grad_norm": 0.09785290062427521,
      "learning_rate": 1.066930801229941e-05,
      "loss": 0.6517,
      "num_input_tokens_seen": 182792768,
      "step": 855
    },
    {
      "epoch": 6.939271255060729,
      "grad_norm": 0.09811808913946152,
      "learning_rate": 1.0408877115918319e-05,
      "loss": 0.646,
      "num_input_tokens_seen": 183907760,
      "step": 860
    }
  ],
  "logging_steps": 5,
  "max_steps": 1230,
  "num_input_tokens_seen": 184122648,
  "num_train_epochs": 10,
  "save_steps": 123,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8.40930757018701e+18,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
